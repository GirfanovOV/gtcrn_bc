{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fb4ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download in streaming mode to take only a part\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "# Fast, good-quality resampling (preferred)\n",
    "import torch\n",
    "import torchaudio.functional as F\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "REPO = \"Cnam-LMSSC/vibravox\"\n",
    "CONFIG = \"speech_clean\"\n",
    "SPLIT = \"test\"\n",
    "\n",
    "N = 1_000\n",
    "\n",
    "SRC_SR = 48_000\n",
    "TGT_SR = 16_000\n",
    "SECONDS = 2.0\n",
    "TGT_LEN = int(TGT_SR * SECONDS)  # 32000\n",
    "\n",
    "# Save space: FLAC (lossless) + int16\n",
    "AUDIO_EXT = \"flac\"               # \"flac\" recommended\n",
    "SF_FORMAT = \"FLAC\"               # if AUDIO_EXT=\"wav\", change to \"WAV\"\n",
    "SF_SUBTYPE = \"PCM_16\"\n",
    "\n",
    "# Which modalities you want to store\n",
    "FIELDS = {\n",
    "    \"headset\": \"audio.headset_microphone\",\n",
    "    \"forehead\": \"audio.forehead_accelerometer\",\n",
    "    \"temple\": \"audio.temple_vibration_pickup\",\n",
    "}\n",
    "\n",
    "out_dir = Path(f\"vibravox_subset_n{N}_sr{TGT_SR}_len{TGT_LEN}_test\")\n",
    "audio_dir = out_dir / \"audio\"\n",
    "audio_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "def _to_mono_float32(samples) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    torchcodec AudioSamples -> mono float32 torch.Tensor [T]\n",
    "    Handles [C, T] and [T].\n",
    "    \"\"\"\n",
    "    x = samples.data  # torch Tensor\n",
    "    if x.ndim == 2:\n",
    "        # average channels to mono (safe default)\n",
    "        x = x.mean(dim=0)\n",
    "    x = x.to(dtype=torch.float32, device=\"cpu\").contiguous()\n",
    "    return x\n",
    "\n",
    "\n",
    "def _resample_48k_to_16k(x_48k: torch.Tensor) -> torch.Tensor:\n",
    "    # torchaudio expects [C, T]\n",
    "    x = x_48k.unsqueeze(0)  # [1, T]\n",
    "    y = F.resample(x, orig_freq=SRC_SR, new_freq=TGT_SR)  # [1, T']\n",
    "    return y.squeeze(0).contiguous()\n",
    "\n",
    "\n",
    "def _trim_or_pad(x: torch.Tensor, length: int) -> torch.Tensor:\n",
    "    n = x.numel()\n",
    "    if n >= length:\n",
    "        return x[:length].contiguous()\n",
    "    # pad with zeros at end\n",
    "    out = torch.zeros(length, dtype=x.dtype)\n",
    "    out[:n] = x\n",
    "    return out\n",
    "\n",
    "\n",
    "def _write_audio(path: Path, x_16k_len: torch.Tensor):\n",
    "    # Write int16 FLAC/WAV to save storage\n",
    "    arr = x_16k_len.numpy()\n",
    "    sf.write(\n",
    "        str(path),\n",
    "        arr,\n",
    "        TGT_SR,\n",
    "        format=SF_FORMAT,\n",
    "        subtype=SF_SUBTYPE,\n",
    "    )\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Stream + process\n",
    "# ----------------------------\n",
    "ds_stream = load_dataset(\n",
    "    REPO,\n",
    "    CONFIG,\n",
    "    split=SPLIT,\n",
    "    streaming=True,\n",
    ")\n",
    "\n",
    "rows = []\n",
    "for ex in tqdm(ds_stream.take(N), total=N):\n",
    "    speaker_id = ex[\"speaker_id\"]\n",
    "    sentence_id = ex[\"sentence_id\"]\n",
    "    duration = float(ex.get(\"duration\", np.nan))\n",
    "\n",
    "    stem = f\"{speaker_id}_{sentence_id}\"\n",
    "\n",
    "    out_paths = {}\n",
    "    # Decode once per field, resample, trim/pad, write\n",
    "    for short, field in FIELDS.items():\n",
    "        samples = ex[field].get_all_samples()  # torchcodec decode\n",
    "        # Optional safety check (dataset is expected 48k)\n",
    "        sr = int(samples.sample_rate)\n",
    "        if sr != SRC_SR:\n",
    "            raise ValueError(f\"Unexpected sample_rate={sr} for {field} (expected {SRC_SR})\")\n",
    "\n",
    "        x = _to_mono_float32(samples)\n",
    "        x = _resample_48k_to_16k(x)\n",
    "        x = _trim_or_pad(x, TGT_LEN)\n",
    "\n",
    "        p = audio_dir / f\"{stem}_{short}.{AUDIO_EXT}\"\n",
    "        _write_audio(p, x)\n",
    "        out_paths[f\"{short}_path\"] = str(p)\n",
    "\n",
    "    rows.append(\n",
    "        {\n",
    "            \"speaker_id\": speaker_id,\n",
    "            \"sentence_id\": sentence_id,\n",
    "            \"duration\": duration,\n",
    "            **out_paths,\n",
    "            \"sr\": TGT_SR,\n",
    "            \"length\": TGT_LEN,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Metadata\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_parquet(out_dir / \"metadata.parquet\", index=False)\n",
    "\n",
    "print(f\"Saved {len(df)} items to: {out_dir.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b08d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# push to hf\n",
    "import os\n",
    "from datasets import load_dataset, Audio, DatasetDict, Value\n",
    "\n",
    "train_dir = \"vibravox_subset_n5000_sr16000_len32000\"\n",
    "test_dir  = \"vibravox_subset_n1000_sr16000_len32000_test\"\n",
    "\n",
    "# 1) load metadata as *Dataset* (not DatasetDict)\n",
    "train = load_dataset(\"parquet\", data_files=f\"{train_dir}/metadata.parquet\", split=\"train\")\n",
    "test  = load_dataset(\"parquet\", data_files=f\"{test_dir}/metadata.parquet\",  split=\"train\")\n",
    "\n",
    "# 2) cast path columns -> string (obligatory) -> Audio\n",
    "audio16 = Audio(sampling_rate=16_000)\n",
    "\n",
    "for col in [\"headset_path\", \"forehead_path\", \"temple_path\"]:\n",
    "    train = train.cast_column(col, Value(\"string\"))\n",
    "    train = train.cast_column(col, audio16)\n",
    "\n",
    "    test  = test.cast_column(col, Value(\"string\"))\n",
    "    test  = test.cast_column(col, audio16)\n",
    "\n",
    "ds = DatasetDict({\"train\": train, \"test\": test})\n",
    "\n",
    "# 3) push\n",
    "repo_id = \"verbreb/vibravox_16k_2s_subset\"\n",
    "ds.push_to_hub(repo_id, embed_external_files=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376c39e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to load dataset\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"verbreb/vibravox_16k_2s_subset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
